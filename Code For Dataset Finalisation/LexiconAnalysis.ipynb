{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ae2fae-9e41-4fd9-b2cd-60f64f7b6832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL OR JOBS LEXICON ANALYSIS - ALL 5 DATASETS COMBINED\n",
      "======================================================================\n",
      "This will process and combine all your OR job datasets with\n",
      "comprehensive lexicon-based gender analysis.\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMBINING ALL OR JOB DATASETS WITH LEXICON ANALYSIS\n",
      "======================================================================\n",
      "Reading Complete_Project_Dataset...\n",
      "File: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Dataset\\Complete Project Dataset.xlsx\n",
      "‚úÖ Successfully read Complete_Project_Dataset!\n",
      "   Shape: (1022, 12)\n",
      "   Columns: ['category', 'city', 'company_name', 'geo', 'job_board', 'job_description', 'job_requirements', 'job_title', 'job_type', 'post_date', 'salary_offered', 'state']\n",
      "\n",
      "Adding lexicon analysis to Complete_Project_Dataset...\n",
      "  Calculating lexicon scores...\n",
      "‚úÖ Lexicon analysis added to Complete_Project_Dataset\n",
      "‚úÖ Processed Complete_Project_Dataset: 1022 jobs\n",
      "Reading Indeed_UK...\n",
      "File: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\indeed_uk_jobs.xlsx\n",
      "‚úÖ Successfully read Indeed_UK!\n",
      "   Shape: (81, 25)\n",
      "   Columns: ['job_id', 'platform', 'job_url', 'scrape_date', 'job_title', 'company_name', 'location', 'job_type', 'salary_displayed', 'posted_date', 'company_rating', 'remote_option', 'job_description_raw', 'job_description_cleaned', 'salary_extracted', 'industry', 'job_level', 'word_count', 'manual_review_flag', 'company_size', 'benefits', 'masculine_score', 'feminine_score', 'neutral_score', 'gendered_words_found']\n",
      "\n",
      "Adding lexicon analysis to Indeed_UK...\n",
      "  Calculating lexicon scores...\n",
      "‚úÖ Lexicon analysis added to Indeed_UK\n",
      "‚úÖ Processed Indeed_UK: 81 jobs\n",
      "Reading OR_Society...\n",
      "File: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\or_society_jobs.xlsx\n",
      "‚úÖ Successfully read OR_Society!\n",
      "   Shape: (60, 22)\n",
      "   Columns: ['job_id', 'platform', 'job_url', 'scrape_date', 'job_title', 'company_name', 'location', 'remote_option', 'posting_date', 'job_description_raw', 'job_description_cleaned', 'salary_info', 'industry', 'job_level', 'gendered_words_found', 'masculine_score', 'feminine_score', 'neutral_score', 'seniority_flag', 'word_count', 'manual_review_flag', 'ad_url']\n",
      "\n",
      "Adding lexicon analysis to OR_Society...\n",
      "  Calculating lexicon scores...\n",
      "‚úÖ Lexicon analysis added to OR_Society\n",
      "‚úÖ Processed OR_Society: 60 jobs\n",
      "Reading Jobs_AC_UK...\n",
      "File: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\jobs_ac_uk_data.xlsx\n",
      "‚úÖ Successfully read Jobs_AC_UK!\n",
      "   Shape: (60, 29)\n",
      "   Columns: ['job_id', 'platform', 'job_url', 'scrape_date', 'job_title', 'company_name', 'institution', 'department', 'location', 'job_type', 'remote_option', 'posting_date', 'job_description_raw', 'job_description_cleaned', 'salary_info', 'industry', 'job_level', 'gendered_words_found', 'masculine_score', 'feminine_score', 'neutral_score', 'seniority_flag', 'word_count', 'manual_review_flag', 'ad_url', 'employer_name', 'closing_date', 'contract_type', 'industry_sector']\n",
      "\n",
      "Adding lexicon analysis to Jobs_AC_UK...\n",
      "  Calculating lexicon scores...\n",
      "‚úÖ Lexicon analysis added to Jobs_AC_UK\n",
      "‚úÖ Processed Jobs_AC_UK: 60 jobs\n",
      "Reading LinkedIn_Jobs...\n",
      "File: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\linkedin_jobs_data.xlsx\n",
      "‚úÖ Successfully read LinkedIn_Jobs!\n",
      "   Shape: (10, 26)\n",
      "   Columns: ['job_id', 'platform', 'job_url', 'scrape_date', 'job_title', 'company_name', 'location', 'remote_option', 'posting_date', 'employment_type', 'experience_level', 'workplace_type', 'job_description_raw', 'job_description_cleaned', 'salary_info', 'industry', 'company_size', 'job_level', 'gendered_words_found', 'masculine_score', 'feminine_score', 'neutral_score', 'seniority_flag', 'word_count', 'manual_review_flag', 'ad_url']\n",
      "\n",
      "Adding lexicon analysis to LinkedIn_Jobs...\n",
      "  Calculating lexicon scores...\n",
      "‚úÖ Lexicon analysis added to LinkedIn_Jobs\n",
      "‚úÖ Processed LinkedIn_Jobs: 10 jobs\n",
      "\n",
      "Combining 5 datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_49776\\3329658566.py:373: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(processed_datasets, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataset shape: (1233, 57)\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE COMBINED DATASET ANALYSIS\n",
      "======================================================================\n",
      "üìä Total jobs analyzed: 1,233\n",
      "\n",
      "üìÅ Jobs by Source:\n",
      "   Complete_Project_Dataset: 1,022 (82.9%)\n",
      "   Indeed_UK: 81 (6.6%)\n",
      "   OR_Society: 60 (4.9%)\n",
      "   Jobs_AC_UK: 60 (4.9%)\n",
      "   LinkedIn_Jobs: 10 (0.8%)\n",
      "\n",
      "üè∑Ô∏è  Overall Gender Classification:\n",
      "   Neutral: 769 (62.4%)\n",
      "   Masculine-coded: 277 (22.5%)\n",
      "   Feminine-coded: 187 (15.2%)\n",
      "\n",
      "üìà Overall Average Lexicon Scores:\n",
      "   Masculine score: 1.71%\n",
      "   Feminine score: 1.50%\n",
      "   Neutral score: 96.79%\n",
      "   Gender ratio: 0.21\n",
      "\n",
      "üìä Gender Classification by Source:\n",
      "\n",
      "   Complete_Project_Dataset:\n",
      "     Feminine-coded: 13.1%\n",
      "     Masculine-coded: 24.7%\n",
      "     Neutral: 62.2%\n",
      "\n",
      "   Indeed_UK:\n",
      "     Feminine-coded: 23.5%\n",
      "     Masculine-coded: 14.8%\n",
      "     Neutral: 61.7%\n",
      "\n",
      "   Jobs_AC_UK:\n",
      "     Feminine-coded: 26.7%\n",
      "     Masculine-coded: 5.0%\n",
      "     Neutral: 68.3%\n",
      "\n",
      "   LinkedIn_Jobs:\n",
      "     Feminine-coded: 50.0%\n",
      "     Masculine-coded: 10.0%\n",
      "     Neutral: 40.0%\n",
      "\n",
      "   OR_Society:\n",
      "     Feminine-coded: 21.7%\n",
      "     Masculine-coded: 15.0%\n",
      "     Neutral: 63.3%\n",
      "\n",
      "üìù Text Analysis:\n",
      "   Average words per job: 368.2\n",
      "   Total words analyzed: 454,025\n",
      "   Jobs with <50 words: 2\n",
      "   Jobs with 50+ words: 1231\n",
      "\n",
      "üíæ Saving final results...\n",
      "‚úÖ Final results saved to: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Dataset\\Final_Combined_OR_Jobs_with_Lexicon_Analysis.xlsx\n",
      "\n",
      "üéâ ANALYSIS COMPLETE!\n",
      "üìÅ Final results saved to: C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Dataset\\Final_Combined_OR_Jobs_with_Lexicon_Analysis.xlsx\n",
      "üìä Total jobs processed: 1,233\n",
      "\n",
      "üìã Sample Results:\n",
      "          dataset_source                                                             job_title               company_name lexicon_gender_classification  lexicon_masculine_score  lexicon_feminine_score  lexicon_gender_ratio\n",
      "Complete_Project_Dataset                                                    Big Data Scientist                  Vodafone                        Neutral                     1.07                    1.88                 -0.81\n",
      "Complete_Project_Dataset Data Scientist / Data Analytics Solution Consultant - Digital Finance                 Edengrove                        Neutral                     1.63                    1.33                  0.30\n",
      "Complete_Project_Dataset                                          Data Scientist / AI Engineer Deerfoot IT Resources Ltd                        Neutral                     0.65                    0.87                 -0.22\n",
      "Complete_Project_Dataset                                                Senior Pricing Analyst              Lowell Group                        Neutral                     1.82                    1.56                  0.26\n",
      "Complete_Project_Dataset                 Quantitative Services Market Risk Senior Data Analyst           Bank of America                        Neutral                     2.32                    2.69                 -0.37\n",
      "Complete_Project_Dataset                 Quantitative Services Market Risk Senior Data Analyst           Bank of America                        Neutral                     2.32                    2.69                 -0.37\n",
      "Complete_Project_Dataset                 Quantitative Services Market Risk Senior Data Analyst           Bank of America                        Neutral                     2.22                    2.61                 -0.39\n",
      "Complete_Project_Dataset                                                      Big Data Analyst                  Vodafone                        Neutral                     1.72                    2.23                 -0.51\n",
      "Complete_Project_Dataset                                                      Big Data Analyst                  Vodafone                        Neutral                     1.72                    2.23                 -0.51\n",
      "Complete_Project_Dataset                                                 Pricing Model Analyst              Lowell Group                        Neutral                     1.41                    1.84                 -0.43\n",
      "\n",
      "üèÜ SUMMARY:\n",
      "   Neutral: 769 jobs (62.4%)\n",
      "   Masculine-coded: 277 jobs (22.5%)\n",
      "   Feminine-coded: 187 jobs (15.2%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class FinalORJobLexiconAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with comprehensive gendered word lexicons matching your survey\"\"\"\n",
    "        \n",
    "        # Masculine-coded words (from your survey and Gaucher et al. 2011)\n",
    "        self.masculine_words = [\n",
    "            # Core masculine terms from survey\n",
    "            'competitive', 'lead', 'result-oriented', 'analytical', 'objective',\n",
    "            'independent', 'autonomous', 'driven', 'decision-making', 'confident',\n",
    "            'self-sufficient',\n",
    "            \n",
    "            # Extended masculine terms from your other datasets\n",
    "            'leader', 'dominant', 'assertive', 'aggressive', 'ambitious',\n",
    "            'decisive', 'determined', 'strong', 'superior', 'manage', 'direct', \n",
    "            'control', 'drive', 'challenge', 'compete', 'win', 'achieve', \n",
    "            'dominate', 'excel', 'individual', 'hierarchy', 'decision', \n",
    "            'responsibility', 'active', 'adventurous', 'athletic', 'battle', \n",
    "            'boast', 'champion', 'courageous', 'defend', 'determine', 'fearless',\n",
    "            'fight', 'force', 'greedy', 'headstrong', 'hostile', 'impulsive',\n",
    "            'intellect', 'logic', 'masculine', 'outgoing', 'outspoken', 'persist',\n",
    "            'principle', 'reckless', 'self-confident', 'self-reliant', 'strength',\n",
    "            'stubborn', 'unreasonable', 'winner', 'results-driven', 'performance',\n",
    "            'efficiency', 'optimization', 'strategic', 'tactical', 'execution',\n",
    "            'implementation', 'delivery', 'targets', 'metrics'\n",
    "        ]\n",
    "        \n",
    "        # Feminine-coded words (from your survey and research)\n",
    "        self.feminine_words = [\n",
    "            # Core feminine terms from survey\n",
    "            'inclusive', 'collaborate', 'responsive', 'nurturing', 'empathetic',\n",
    "            'compassionate', 'ambitious', 'motivated', 'team-player', 'interpersonal-skills',\n",
    "            'support',\n",
    "            \n",
    "            # Extended feminine terms from your other datasets\n",
    "            'collaborative', 'cooperative', 'supportive', 'communicate', 'understand', \n",
    "            'responsible', 'connect', 'honest', 'loyal', 'dependable', 'committed', \n",
    "            'dedicated', 'help', 'assist', 'care', 'share', 'together', 'team', \n",
    "            'community', 'relationship', 'trust', 'warm', 'kind', 'agree', \n",
    "            'affectionate', 'child', 'cheer', 'collab', 'commit', 'communal', \n",
    "            'compassion', 'considerate', 'cooperate', 'co-operate', 'depend',\n",
    "            'emotional', 'empathy', 'feel', 'feeling', 'feminine', 'flatterable',\n",
    "            'gentle', 'interdependent', 'interpersonal', 'intimate', 'kinship',\n",
    "            'modesty', 'nag', 'nice', 'nurture', 'pleasant', 'polite', 'quiet',\n",
    "            'sensitive', 'submissive', 'sympathy', 'tender', 'whiny', 'yield',\n",
    "            'caring', 'helping', 'communication', 'stakeholder', 'facilitation',\n",
    "            'coordination', 'consultation', 'liaison', 'partnership', 'consensus',\n",
    "            'engagement'\n",
    "        ]\n",
    "        \n",
    "        # Fast-paced/High-performing terms (can be considered masculine-leaning)\n",
    "        self.fast_paced_words = [\n",
    "            'fast-paced', 'high-performing', 'fast', 'rapid', 'quick', 'speed',\n",
    "            'urgent', 'immediate', 'accelerated', 'dynamic', 'intense'\n",
    "        ]\n",
    "        \n",
    "        # Add fast-paced terms to masculine list\n",
    "        self.masculine_words.extend(self.fast_paced_words)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text for analysis\"\"\"\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Handle compound words and hyphenated terms\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace('_', ' ')\n",
    "        \n",
    "        # Remove extra whitespace and normalize\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove special characters but keep important punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def calculate_lexicon_scores(self, text):\n",
    "        \"\"\"Calculate gendered language scores using lexicon approach\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'masculine_count': 0,\n",
    "                'feminine_count': 0,\n",
    "                'masculine_score': 0.0,\n",
    "                'feminine_score': 0.0,\n",
    "                'neutral_score': 100.0,\n",
    "                'gender_ratio': 0.0,\n",
    "                'gender_classification': 'Neutral',\n",
    "                'total_words': 0,\n",
    "                'gendered_words_found': ''\n",
    "            }\n",
    "        \n",
    "        # Clean and tokenize\n",
    "        clean_text = self.clean_text(text)\n",
    "        words = re.findall(r'\\b[a-z]+\\b', clean_text)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return {\n",
    "                'masculine_count': 0,\n",
    "                'feminine_count': 0,\n",
    "                'masculine_score': 0.0,\n",
    "                'feminine_score': 0.0,\n",
    "                'neutral_score': 100.0,\n",
    "                'gender_ratio': 0.0,\n",
    "                'gender_classification': 'Neutral',\n",
    "                'total_words': 0,\n",
    "                'gendered_words_found': ''\n",
    "            }\n",
    "        \n",
    "        # Count gendered words\n",
    "        masculine_words_found = []\n",
    "        feminine_words_found = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.masculine_words:\n",
    "                masculine_words_found.append(word)\n",
    "            elif word in self.feminine_words:\n",
    "                feminine_words_found.append(word)\n",
    "        \n",
    "        masculine_count = len(masculine_words_found)\n",
    "        feminine_count = len(feminine_words_found)\n",
    "        \n",
    "        # Calculate scores as percentage of total words\n",
    "        masculine_score = round((masculine_count / total_words) * 100, 2)\n",
    "        feminine_score = round((feminine_count / total_words) * 100, 2)\n",
    "        neutral_score = round(100 - masculine_score - feminine_score, 2)\n",
    "        \n",
    "        # Calculate gender ratio (masculine - feminine)\n",
    "        gender_ratio = round(masculine_score - feminine_score, 2)\n",
    "        \n",
    "        # Classify based on threshold (following academic literature)\n",
    "        if gender_ratio > 1.0:\n",
    "            classification = 'Masculine-coded'\n",
    "        elif gender_ratio < -1.0:\n",
    "            classification = 'Feminine-coded'\n",
    "        else:\n",
    "            classification = 'Neutral'\n",
    "        \n",
    "        # Create summary of found words\n",
    "        gendered_words_summary = []\n",
    "        if masculine_words_found:\n",
    "            unique_masc = list(set(masculine_words_found))\n",
    "            gendered_words_summary.append(f\"M: {', '.join(unique_masc)}\")\n",
    "        if feminine_words_found:\n",
    "            unique_fem = list(set(feminine_words_found))\n",
    "            gendered_words_summary.append(f\"F: {', '.join(unique_fem)}\")\n",
    "        \n",
    "        return {\n",
    "            'masculine_count': masculine_count,\n",
    "            'feminine_count': feminine_count,\n",
    "            'masculine_score': masculine_score,\n",
    "            'feminine_score': feminine_score,\n",
    "            'neutral_score': neutral_score,\n",
    "            'gender_ratio': gender_ratio,\n",
    "            'gender_classification': classification,\n",
    "            'total_words': total_words,\n",
    "            'gendered_words_found': ' | '.join(gendered_words_summary)\n",
    "        }\n",
    "\n",
    "    def read_dataset(self, file_path, dataset_name):\n",
    "        \"\"\"Read dataset with proper encoding handling\"\"\"\n",
    "        \n",
    "        print(f\"Reading {dataset_name}...\")\n",
    "        print(f\"File: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Determine file type and read accordingly\n",
    "            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "                df = pd.read_excel(file_path, engine='openpyxl')\n",
    "            else:\n",
    "                # Try different encodings for CSV\n",
    "                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "                df = None\n",
    "                \n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, encoding=encoding, low_memory=False)\n",
    "                        print(f\"  Successfully read with {encoding} encoding\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if df is None:\n",
    "                    raise ValueError(f\"Could not read {file_path} with any encoding\")\n",
    "            \n",
    "            print(f\"‚úÖ Successfully read {dataset_name}!\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Add dataset identifier\n",
    "            df['dataset_source'] = dataset_name\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def standardize_complete_dataset(self, df):\n",
    "        \"\"\"Standardize the Complete Project Dataset\"\"\"\n",
    "        \n",
    "        # Create standardized columns\n",
    "        standardized_df = df.copy()\n",
    "        \n",
    "        # Map columns to standard names\n",
    "        column_mapping = {\n",
    "            'job_title': 'job_title',\n",
    "            'company_name': 'company_name', \n",
    "            'city': 'location',\n",
    "            'category': 'category',\n",
    "            'job_board_description': 'job_description',\n",
    "            'requirements': 'requirements',\n",
    "            'post_date': 'posting_date',\n",
    "            'salary_offered': 'salary_info'\n",
    "        }\n",
    "        \n",
    "        # Apply mappings where columns exist\n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in df.columns and old_col != new_col:\n",
    "                standardized_df = standardized_df.rename(columns={old_col: new_col})\n",
    "        \n",
    "        # Create combined text for analysis\n",
    "        text_columns = ['job_title', 'job_description', 'requirements']\n",
    "        combined_text = []\n",
    "        \n",
    "        for idx, row in standardized_df.iterrows():\n",
    "            text_parts = []\n",
    "            \n",
    "            for col in text_columns:\n",
    "                if col in standardized_df.columns and pd.notna(row[col]):\n",
    "                    text_parts.append(str(row[col]))\n",
    "            \n",
    "            combined_text.append(' '.join(text_parts))\n",
    "        \n",
    "        standardized_df['combined_text_for_analysis'] = combined_text\n",
    "        \n",
    "        return standardized_df\n",
    "\n",
    "    def standardize_other_datasets(self, df, dataset_name):\n",
    "        \"\"\"Standardize other datasets to match Complete Project format\"\"\"\n",
    "        \n",
    "        standardized_df = df.copy()\n",
    "        \n",
    "        # Common standardization based on dataset type\n",
    "        if dataset_name == 'Indeed_UK':\n",
    "            if 'job_description_cleaned' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_cleaned']\n",
    "            elif 'job_description_raw' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_raw']\n",
    "        \n",
    "        elif dataset_name == 'OR_Society':\n",
    "            if 'job_description_cleaned' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_cleaned']\n",
    "            elif 'job_description_raw' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_raw']\n",
    "        \n",
    "        elif dataset_name == 'Jobs_AC_UK':\n",
    "            if 'job_description_cleaned' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_cleaned']\n",
    "            elif 'job_description_raw' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_raw']\n",
    "        \n",
    "        elif dataset_name == 'LinkedIn_Jobs':\n",
    "            if 'job_description_cleaned' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_cleaned']\n",
    "            elif 'job_description_raw' in df.columns:\n",
    "                standardized_df['combined_text_for_analysis'] = df['job_description_raw']\n",
    "        \n",
    "        # If no specific text column found, try to create one\n",
    "        if 'combined_text_for_analysis' not in standardized_df.columns:\n",
    "            # Look for common text columns\n",
    "            text_cols = ['job_title', 'description', 'job_description', 'summary']\n",
    "            combined_text = []\n",
    "            \n",
    "            for idx, row in standardized_df.iterrows():\n",
    "                text_parts = []\n",
    "                for col in text_cols:\n",
    "                    if col in standardized_df.columns and pd.notna(row[col]):\n",
    "                        text_parts.append(str(row[col]))\n",
    "                combined_text.append(' '.join(text_parts))\n",
    "            \n",
    "            standardized_df['combined_text_for_analysis'] = combined_text\n",
    "        \n",
    "        return standardized_df\n",
    "\n",
    "    def add_lexicon_analysis_to_dataset(self, df, dataset_name):\n",
    "        \"\"\"Add lexicon analysis to any dataset\"\"\"\n",
    "        \n",
    "        print(f\"\\nAdding lexicon analysis to {dataset_name}...\")\n",
    "        \n",
    "        # Ensure we have text to analyze\n",
    "        if 'combined_text_for_analysis' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  No analysis text found for {dataset_name}\")\n",
    "            return df\n",
    "        \n",
    "        # Apply lexicon analysis\n",
    "        print(f\"  Calculating lexicon scores...\")\n",
    "        \n",
    "        lexicon_results = df['combined_text_for_analysis'].fillna('').apply(self.calculate_lexicon_scores)\n",
    "        \n",
    "        # Extract results into separate columns\n",
    "        for key in ['masculine_count', 'feminine_count', 'masculine_score', \n",
    "                   'feminine_score', 'neutral_score', 'gender_ratio', \n",
    "                   'gender_classification', 'total_words', 'gendered_words_found']:\n",
    "            df[f'lexicon_{key}'] = [result[key] for result in lexicon_results]\n",
    "        \n",
    "        # Add analysis metadata\n",
    "        df['analysis_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        df['lexicon_version'] = 'Gaucher_2011_Extended_Survey_Based'\n",
    "        \n",
    "        print(f\"‚úÖ Lexicon analysis added to {dataset_name}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def combine_all_datasets(self, dataset_paths):\n",
    "        \"\"\"Combine all 5 datasets with lexicon analysis\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"COMBINING ALL OR JOB DATASETS WITH LEXICON ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        processed_datasets = []\n",
    "        \n",
    "        for dataset_name, file_path in dataset_paths.items():\n",
    "            # Read dataset\n",
    "            df = self.read_dataset(file_path, dataset_name)\n",
    "            \n",
    "            if df is not None:\n",
    "                # Standardize based on dataset type\n",
    "                if dataset_name == 'Complete_Project_Dataset':\n",
    "                    df_standardized = self.standardize_complete_dataset(df)\n",
    "                else:\n",
    "                    df_standardized = self.standardize_other_datasets(df, dataset_name)\n",
    "                \n",
    "                # Add lexicon analysis\n",
    "                df_with_lexicon = self.add_lexicon_analysis_to_dataset(df_standardized, dataset_name)\n",
    "                \n",
    "                processed_datasets.append(df_with_lexicon)\n",
    "                print(f\"‚úÖ Processed {dataset_name}: {len(df_with_lexicon)} jobs\")\n",
    "            else:\n",
    "                print(f\"‚ùå Skipped {dataset_name}: Could not read file\")\n",
    "        \n",
    "        if not processed_datasets:\n",
    "            raise ValueError(\"No datasets could be processed\")\n",
    "        \n",
    "        # Get all unique columns across datasets\n",
    "        all_columns = set()\n",
    "        for df in processed_datasets:\n",
    "            all_columns.update(df.columns)\n",
    "        \n",
    "        # Ensure all dataframes have the same columns (fill missing with None)\n",
    "        for i, df in enumerate(processed_datasets):\n",
    "            for col in all_columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "            processed_datasets[i] = df[list(all_columns)]\n",
    "        \n",
    "        # Combine all datasets\n",
    "        print(f\"\\nCombining {len(processed_datasets)} datasets...\")\n",
    "        combined_df = pd.concat(processed_datasets, ignore_index=True, sort=False)\n",
    "        \n",
    "        print(f\"‚úÖ Combined dataset shape: {combined_df.shape}\")\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def generate_comprehensive_analysis(self, df):\n",
    "        \"\"\"Generate comprehensive analysis of the combined dataset\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPREHENSIVE COMBINED DATASET ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_jobs = len(df)\n",
    "        print(f\"üìä Total jobs analyzed: {total_jobs:,}\")\n",
    "        \n",
    "        # Dataset source breakdown\n",
    "        if 'dataset_source' in df.columns:\n",
    "            print(f\"\\nüìÅ Jobs by Source:\")\n",
    "            source_counts = df['dataset_source'].value_counts()\n",
    "            for source, count in source_counts.items():\n",
    "                percentage = (count / total_jobs) * 100\n",
    "                print(f\"   {source}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Gender classification breakdown\n",
    "        if 'lexicon_gender_classification' in df.columns:\n",
    "            print(f\"\\nüè∑Ô∏è  Overall Gender Classification:\")\n",
    "            gender_counts = df['lexicon_gender_classification'].value_counts()\n",
    "            for classification, count in gender_counts.items():\n",
    "                percentage = (count / total_jobs) * 100\n",
    "                print(f\"   {classification}: {count:,} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Average scores\n",
    "            print(f\"\\nüìà Overall Average Lexicon Scores:\")\n",
    "            print(f\"   Masculine score: {df['lexicon_masculine_score'].mean():.2f}%\")\n",
    "            print(f\"   Feminine score: {df['lexicon_feminine_score'].mean():.2f}%\")\n",
    "            print(f\"   Neutral score: {df['lexicon_neutral_score'].mean():.2f}%\")\n",
    "            print(f\"   Gender ratio: {df['lexicon_gender_ratio'].mean():.2f}\")\n",
    "            \n",
    "            # Gender analysis by source\n",
    "            if 'dataset_source' in df.columns:\n",
    "                print(f\"\\nüìä Gender Classification by Source:\")\n",
    "                gender_by_source = pd.crosstab(\n",
    "                    df['dataset_source'], \n",
    "                    df['lexicon_gender_classification'], \n",
    "                    normalize='index'\n",
    "                ) * 100\n",
    "                \n",
    "                for source in gender_by_source.index:\n",
    "                    print(f\"\\n   {source}:\")\n",
    "                    for gender_class in gender_by_source.columns:\n",
    "                        percentage = gender_by_source.loc[source, gender_class]\n",
    "                        print(f\"     {gender_class}: {percentage:.1f}%\")\n",
    "        \n",
    "        # Word analysis\n",
    "        if 'lexicon_total_words' in df.columns:\n",
    "            print(f\"\\nüìù Text Analysis:\")\n",
    "            print(f\"   Average words per job: {df['lexicon_total_words'].mean():.1f}\")\n",
    "            print(f\"   Total words analyzed: {df['lexicon_total_words'].sum():,}\")\n",
    "            print(f\"   Jobs with <50 words: {(df['lexicon_total_words'] < 50).sum()}\")\n",
    "            print(f\"   Jobs with 50+ words: {(df['lexicon_total_words'] >= 50).sum()}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def save_final_results(self, df, output_path):\n",
    "        \"\"\"Save final combined results with comprehensive analysis\"\"\"\n",
    "        \n",
    "        print(f\"\\nüíæ Saving final results...\")\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                # Main combined dataset\n",
    "                df.to_excel(writer, sheet_name='All_Jobs_Combined', index=False)\n",
    "                \n",
    "                # Summary statistics\n",
    "                summary_data = []\n",
    "                if 'lexicon_gender_classification' in df.columns:\n",
    "                    gender_counts = df['lexicon_gender_classification'].value_counts()\n",
    "                    source_counts = df['dataset_source'].value_counts() if 'dataset_source' in df.columns else {}\n",
    "                    \n",
    "                    summary_data = [\n",
    "                        ['Total Jobs', len(df)],\n",
    "                        ['Total Datasets', df['dataset_source'].nunique() if 'dataset_source' in df.columns else 'N/A'],\n",
    "                        ['Masculine-coded Jobs', gender_counts.get('Masculine-coded', 0)],\n",
    "                        ['Feminine-coded Jobs', gender_counts.get('Feminine-coded', 0)],\n",
    "                        ['Neutral Jobs', gender_counts.get('Neutral', 0)],\n",
    "                        ['Avg Masculine Score (%)', round(df['lexicon_masculine_score'].mean(), 2)],\n",
    "                        ['Avg Feminine Score (%)', round(df['lexicon_feminine_score'].mean(), 2)],\n",
    "                        ['Avg Neutral Score (%)', round(df['lexicon_neutral_score'].mean(), 2)],\n",
    "                        ['Avg Gender Ratio', round(df['lexicon_gender_ratio'].mean(), 2)],\n",
    "                        ['Total Words Analyzed', df['lexicon_total_words'].sum() if 'lexicon_total_words' in df.columns else 'N/A']\n",
    "                    ]\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Value'])\n",
    "                summary_df.to_excel(writer, sheet_name='Overall_Summary', index=False)\n",
    "                \n",
    "                # Source breakdown\n",
    "                if 'dataset_source' in df.columns:\n",
    "                    source_breakdown = df['dataset_source'].value_counts().reset_index()\n",
    "                    source_breakdown.columns = ['Dataset_Source', 'Job_Count']\n",
    "                    source_breakdown['Percentage'] = (source_breakdown['Job_Count'] / len(df) * 100).round(1)\n",
    "                    source_breakdown.to_excel(writer, sheet_name='Source_Breakdown', index=False)\n",
    "                \n",
    "                # Gender analysis by source\n",
    "                if 'dataset_source' in df.columns and 'lexicon_gender_classification' in df.columns:\n",
    "                    gender_by_source = pd.crosstab(\n",
    "                        df['dataset_source'], \n",
    "                        df['lexicon_gender_classification']\n",
    "                    ).reset_index()\n",
    "                    gender_by_source.to_excel(writer, sheet_name='Gender_by_Source', index=False)\n",
    "                    \n",
    "                    # Percentage breakdown\n",
    "                    gender_by_source_pct = pd.crosstab(\n",
    "                        df['dataset_source'], \n",
    "                        df['lexicon_gender_classification'],\n",
    "                        normalize='index'\n",
    "                    ).round(3) * 100\n",
    "                    gender_by_source_pct.to_excel(writer, sheet_name='Gender_by_Source_Percent')\n",
    "                \n",
    "                # Top gendered words analysis\n",
    "                if 'lexicon_gendered_words_found' in df.columns:\n",
    "                    all_gendered_words = []\n",
    "                    for words_str in df['lexicon_gendered_words_found'].dropna():\n",
    "                        if words_str:\n",
    "                            parts = words_str.split(' | ')\n",
    "                            for part in parts:\n",
    "                                if part.startswith('M: '):\n",
    "                                    words = part[3:].split(', ')\n",
    "                                    for word in words:\n",
    "                                        all_gendered_words.append(('Masculine', word.strip()))\n",
    "                                elif part.startswith('F: '):\n",
    "                                    words = part[3:].split(', ')\n",
    "                                    for word in words:\n",
    "                                        all_gendered_words.append(('Feminine', word.strip()))\n",
    "                    \n",
    "                    if all_gendered_words:\n",
    "                        words_df = pd.DataFrame(all_gendered_words, columns=['Type', 'Word'])\n",
    "                        word_counts = words_df.groupby(['Type', 'Word']).size().reset_index(name='Count')\n",
    "                        word_counts = word_counts.sort_values(['Type', 'Count'], ascending=[True, False])\n",
    "                        word_counts.to_excel(writer, sheet_name='Top_Gendered_Words', index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Final results saved to: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all datasets and combine with lexicon analysis\"\"\"\n",
    "    \n",
    "    # Define all dataset paths\n",
    "    dataset_paths = {\n",
    "        'Complete_Project_Dataset': r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Dataset\\Complete Project Dataset.xlsx\",\n",
    "        'Indeed_UK': r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\indeed_uk_jobs.xlsx\",\n",
    "        'OR_Society': r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\or_society_jobs.xlsx\", \n",
    "        'Jobs_AC_UK': r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\jobs_ac_uk_data.xlsx\",\n",
    "        'LinkedIn_Jobs': r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Codes\\linkedin_jobs_data.xlsx\"\n",
    "    }\n",
    "    \n",
    "    # Output path\n",
    "    output_path = r\"C:\\Users\\HP\\OneDrive - University of Southampton\\Documents\\Dissertation Project - Marwa Ashfaq\\Dataset\\Final_Combined_OR_Jobs_with_Lexicon_Analysis.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\"*70)\n",
    "        print(\"FINAL OR JOBS LEXICON ANALYSIS - ALL 5 DATASETS COMBINED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"This will process and combine all your OR job datasets with\")\n",
    "        print(\"comprehensive lexicon-based gender analysis.\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize analyzer\n",
    "        analyzer = FinalORJobLexiconAnalyzer()\n",
    "        \n",
    "        # Process and combine all datasets\n",
    "        combined_df = analyzer.combine_all_datasets(dataset_paths)\n",
    "        \n",
    "        # Generate comprehensive analysis\n",
    "        final_df = analyzer.generate_comprehensive_analysis(combined_df)\n",
    "        \n",
    "        # Save final results\n",
    "        success = analyzer.save_final_results(final_df, output_path)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "            print(f\"üìÅ Final results saved to: {output_path}\")\n",
    "            print(f\"üìä Total jobs processed: {len(final_df):,}\")\n",
    "            \n",
    "            # Show sample of results\n",
    "            if 'lexicon_gender_classification' in final_df.columns:\n",
    "                print(f\"\\nüìã Sample Results:\")\n",
    "                sample_cols = ['dataset_source', 'job_title', 'company_name', \n",
    "                              'lexicon_gender_classification', 'lexicon_masculine_score', \n",
    "                              'lexicon_feminine_score', 'lexicon_gender_ratio']\n",
    "                available_cols = [col for col in sample_cols if col in final_df.columns]\n",
    "                \n",
    "                sample_data = final_df[available_cols].head(10)\n",
    "                print(sample_data.to_string(index=False))\n",
    "                \n",
    "                print(f\"\\nüèÜ SUMMARY:\")\n",
    "                gender_summary = final_df['lexicon_gender_classification'].value_counts()\n",
    "                for classification, count in gender_summary.items():\n",
    "                    percentage = (count / len(final_df)) * 100\n",
    "                    print(f\"   {classification}: {count:,} jobs ({percentage:.1f}%)\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aeabd6-167c-4cdb-9e89-6ceeaf93f132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
